{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Default Settings",
   "id": "f7986125f474341f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T02:40:52.632160Z",
     "start_time": "2024-12-20T02:40:30.758487Z"
    }
   },
   "source": [
    "# 필요한 라이브러리 가져오기\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import wandb\n",
    "import numpy as np\n",
    "\n",
    "# 모듈 캐시 제거 및 재로드\n",
    "# import importlib\n",
    "# from _01_code._03_real_world_data_to_tensors import p_cryptocurrency_dataset_dataloader as dataloader\n",
    "\n",
    "# importlib.reload(dataloader)\n",
    "\n",
    "# 필요한 함수 가져오기\n",
    "from _01_code._03_real_world_data_to_tensors.p_cryptocurrency_dataset_dataloader import (\n",
    "    get_cryptocurrency_data,\n",
    "    CryptoCurrencyDataset,\n",
    "    add_next_open_column\n",
    ")\n",
    "\n",
    "# 주피터 노트북에서는 os.getcwd()로 현재 작업 디렉토리 설정\n",
    "CURRENT_FILE_PATH = os.getcwd()\n",
    "\n",
    "# 체크포인트 파일 경로 설정\n",
    "CHECKPOINT_FILE_PATH = os.path.join(CURRENT_FILE_PATH, \"checkpoints\")\n",
    "if not os.path.isdir(CHECKPOINT_FILE_PATH):\n",
    "    os.makedirs(CHECKPOINT_FILE_PATH)\n",
    "\n",
    "# Wandb 설정 초기화 (새로운 세션 생성)\n",
    "wandb.init(\n",
    "    project=\"lstm_regression_btc_krw_next_open\",\n",
    "    name=f\"BTC_KRW_Regression_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    mode=\"online\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: yuuun03 (yuuun03-korea-university-of-technology-and-education). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\yscho\\git\\link_dl\\_02_homeworks\\homework_4\\wandb\\run-20241220_114051-7a5gzfdz</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/lstm_regression_btc_krw_next_open/runs/7a5gzfdz' target=\"_blank\">BTC_KRW_Regression_20241220_114049</a></strong> to <a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/lstm_regression_btc_krw_next_open' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/lstm_regression_btc_krw_next_open' target=\"_blank\">https://wandb.ai/yuuun03-korea-university-of-technology-and-education/lstm_regression_btc_krw_next_open</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/lstm_regression_btc_krw_next_open/runs/7a5gzfdz' target=\"_blank\">https://wandb.ai/yuuun03-korea-university-of-technology-and-education/lstm_regression_btc_krw_next_open/runs/7a5gzfdz</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/lstm_regression_btc_krw_next_open/runs/7a5gzfdz?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1da3a8b6860>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "9629c21a038b63dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T02:40:52.679167Z",
     "start_time": "2024-12-20T02:40:52.662734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LSTM 모델 정의\n",
    "class MyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM 기반 회귀 모델 정의.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_output):\n",
    "        \"\"\"\n",
    "        모델 초기화 함수.\n",
    "\n",
    "        Args:\n",
    "            n_input (int): 입력 특성(feature)의 수\n",
    "            n_output (int): 출력 특성(feature)의 수\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # LSTM 레이어: 순차 데이터를 처리하며, 시계열 데이터의 패턴을 학습.\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_input,\n",
    "            hidden_size=256,\n",
    "            num_layers=3,\n",
    "            dropout=0.3,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # Fully Connected Layer: LSTM의 출력을 최종 예측값으로 변환.\n",
    "        self.fcn1 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fcn2 = nn.Linear(in_features=128, out_features=n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        모델의 순전파 연산 정의.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): 입력 데이터 (shape: [batch_size, sequence_length, n_input])\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 예측값 (shape: [batch_size, n_output])\n",
    "        \"\"\"\n",
    "        x, _ = self.lstm(x)  # LSTM 레이어를 통해 출력값 생성\n",
    "        x = x[:, -1, :]  # 시퀀스의 마지막 출력값 선택\n",
    "        x = torch.relu(self.fcn1(x))  # ReLU 활성화 함수 적용\n",
    "        x = self.fcn2(x)  # 최종 예측값\n",
    "        return x"
   ],
   "id": "165949e312559d8f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T02:40:52.709976Z",
     "start_time": "2024-12-20T02:40:52.691815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Backpropagation 함수\n",
    "def backpropagation_step(model, X_batch, y_batch, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Backpropagation 수행 함수\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): LSTM 모델\n",
    "        X_batch (torch.Tensor): 입력 데이터\n",
    "        y_batch (torch.Tensor): 타겟 데이터\n",
    "        optimizer (torch.optim.Optimizer): 옵티마이저\n",
    "        criterion (torch.nn.Module): 손실 함수\n",
    "        device (torch.device): 실행 장치\n",
    "\n",
    "    Returns:\n",
    "        float: 배치 손실 값\n",
    "    \"\"\"\n",
    "    # 데이터 장치로 이동\n",
    "    X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(-1)\n",
    "\n",
    "    # 옵티마이저 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 모델 예측\n",
    "    output = model(X_batch)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = criterion(output, y_batch)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 가중치 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ],
   "id": "6427895fd255a758",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Loader",
   "id": "4195cbc994281755"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T02:40:52.941826Z",
     "start_time": "2024-12-20T02:40:52.928536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_btc_krw_data(sequence_size=10, validation_size=100, test_size=10, is_regression=True):\n",
    "    \"\"\"\n",
    "    데이터셋을 가져와 DataLoader로 변환.\n",
    "\n",
    "    Args:\n",
    "        sequence_size (int): 시퀀스 길이\n",
    "        validation_size (int): 검증 데이터 크기\n",
    "        test_size (int): 테스트 데이터 크기\n",
    "        is_regression (bool): 회귀 문제 여부\n",
    "\n",
    "    Returns:\n",
    "        tuple: 학습, 검증, 테스트 데이터 로더\n",
    "    \"\"\"\n",
    "    # 데이터 가공 (Next_Open 포함)\n",
    "    add_next_open_column()  # BTC_KRW_with_next_open.csv 생성\n",
    "\n",
    "    # 데이터를 로드하고, Next_Open을 피처로 포함.\n",
    "    X_train, X_validation, X_test, y_train, y_validation, y_test, _, _, _ = get_cryptocurrency_data(\n",
    "        sequence_size=sequence_size,\n",
    "        validation_size=validation_size,\n",
    "        test_size=test_size,\n",
    "        target_column=\"Close\",  # 타겟 데이터는 여전히 \"Close\"\n",
    "        y_normalizer=1.0e7,\n",
    "        is_regression=is_regression,\n",
    "        include_next_open=True  # 추가 피처로 Next_Open 포함\n",
    "    )\n",
    "\n",
    "    # 데이터셋 정의\n",
    "    train_dataset = CryptoCurrencyDataset(X=X_train, y=y_train)\n",
    "    validation_dataset = CryptoCurrencyDataset(X=X_validation, y=y_validation)\n",
    "    test_dataset = CryptoCurrencyDataset(X=X_test, y=y_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader"
   ],
   "id": "c252df923cd86c5f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train",
   "id": "3b626fab4a6debf8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T02:40:52.988745Z",
     "start_time": "2024-12-20T02:40:52.961820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, train_loader, validation_loader, device, epochs=10, lr=1e-4):\n",
    "    \"\"\"\n",
    "    모델 학습 함수 (Backpropagation 포함).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): LSTM 모델\n",
    "        train_loader (DataLoader): 학습 데이터 로더\n",
    "        validation_loader (DataLoader): 검증 데이터 로더\n",
    "        device (torch.device): 실행 장치\n",
    "        epochs (int): 학습 에폭 수\n",
    "        lr (float): 학습률\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Backpropagation Step\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(-1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f}\")\n",
    "        wandb.log({\"Train Loss\": train_loss, \"Epoch\": epoch + 1, \"Learning Rate\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "        # 검증 단계\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in validation_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(-1)\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= len(validation_loader)\n",
    "        print(f\"[Epoch {epoch+1}] Validation Loss: {validation_loss:.4f}\")\n",
    "        wandb.log({\"Validation Loss\": validation_loss, \"Epoch\": epoch + 1})\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(CHECKPOINT_FILE_PATH, \"lstm_model_checkpoint.pt\"))\n"
   ],
   "id": "1ed8b6757528a27d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "e0dcabe4c872eff1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T02:40:53.081579Z",
     "start_time": "2024-12-20T02:40:53.055638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    테스트 데이터셋에서 모델 성능 평가.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 학습된 모델\n",
    "        test_loader (DataLoader): 테스트 데이터 로더\n",
    "        device (torch.device): 실행 장치\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions, targets = [], []\n",
    "\n",
    "    print(\"[TEST DATA]\")\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(-1)\n",
    "            output = model(X_batch)\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    test_loss = np.mean(np.abs(np.array(predictions) - np.array(targets)))\n",
    "    print(f\"Test Loss (Average): {test_loss:.2f}\")\n",
    "    wandb.log({\"Test Loss\": test_loss})\n"
   ],
   "id": "ca617b285c23a77e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualization",
   "id": "ea0668c5a8cd68e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T02:40:53.127585Z",
     "start_time": "2024-12-20T02:40:53.099674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 그래프 시각화 함수\n",
    "def predict_all(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    테스트 데이터셋의 예측 결과 시각화\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 학습된 모델\n",
    "        test_loader (DataLoader): 테스트 데이터 로더\n",
    "        device (torch.device): 실행 장치\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    y_normalizer = 100\n",
    "    X, TARGET_Y, PREDICTION_Y = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(-1)\n",
    "            predictions = model(X_batch).squeeze(-1)\n",
    "\n",
    "            X.extend(range(len(y_batch)))\n",
    "            TARGET_Y.extend(y_batch.cpu().numpy() * y_normalizer)\n",
    "            PREDICTION_Y.extend(predictions.cpu().numpy() * y_normalizer)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(X, TARGET_Y, label=\"Actual\")\n",
    "    plt.plot(X, PREDICTION_Y, label=\"Predicted\")\n",
    "    plt.title(\"Regression Results\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    wandb.log({\"Test Visualization\": wandb.Image(plt)})"
   ],
   "id": "7903c18641a0efb0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Main",
   "id": "9c3b84634cc9a886"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T02:44:14.721976Z",
     "start_time": "2024-12-20T02:40:53.150203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 메인 실행 코드\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 데이터 로드\n",
    "    train_loader, validation_loader, test_loader = get_btc_krw_data(sequence_size=10)\n",
    "\n",
    "    # 모델 생성 (n_input=6: 추가 피처 포함)\n",
    "    model = MyModel(n_input=6, n_output=1)\n",
    "\n",
    "    # 모델 학습\n",
    "    train(model, train_loader, validation_loader, device, epochs=50, lr=1e-3)\n",
    "\n",
    "    # 테스트\n",
    "    test(model, test_loader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "2cc431e8bfc79401",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to: C:\\Users\\yscho\\git\\link_dl\\_00_data\\k_cryptocurrency\\BTC_KRW_with_next_open.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yscho\\git\\link_dl\\_01_code\\_03_real_world_data_to_tensors\\p_cryptocurrency_dataset_dataloader.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X, dtype=torch.float32)  # 입력 데이터는 항상 float32로 저장\n",
      "C:\\Users\\yscho\\git\\link_dl\\_01_code\\_03_real_world_data_to_tensors\\p_cryptocurrency_dataset_dataloader.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y, dtype=torch.float32)  # 회귀 문제일 경우 float32로 저장\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.4798\n",
      "[Epoch 1] Validation Loss: 0.0308\n",
      "[Epoch 2] Train Loss: 0.0396\n",
      "[Epoch 2] Validation Loss: 0.0300\n",
      "[Epoch 3] Train Loss: 0.0367\n",
      "[Epoch 3] Validation Loss: 0.0114\n",
      "[Epoch 4] Train Loss: 0.0310\n",
      "[Epoch 4] Validation Loss: 0.0054\n",
      "[Epoch 5] Train Loss: 0.0339\n",
      "[Epoch 5] Validation Loss: 0.0081\n",
      "[Epoch 6] Train Loss: 0.0258\n",
      "[Epoch 6] Validation Loss: 0.0971\n",
      "[Epoch 7] Train Loss: 0.0240\n",
      "[Epoch 7] Validation Loss: 0.0146\n",
      "[Epoch 8] Train Loss: 0.0219\n",
      "[Epoch 8] Validation Loss: 0.0047\n",
      "[Epoch 9] Train Loss: 0.0193\n",
      "[Epoch 9] Validation Loss: 0.0076\n",
      "[Epoch 10] Train Loss: 0.0180\n",
      "[Epoch 10] Validation Loss: 0.0048\n",
      "[Epoch 11] Train Loss: 0.0177\n",
      "[Epoch 11] Validation Loss: 0.0134\n",
      "[Epoch 12] Train Loss: 0.0165\n",
      "[Epoch 12] Validation Loss: 0.0479\n",
      "[Epoch 13] Train Loss: 0.0193\n",
      "[Epoch 13] Validation Loss: 0.0072\n",
      "[Epoch 14] Train Loss: 0.0175\n",
      "[Epoch 14] Validation Loss: 0.0100\n",
      "[Epoch 15] Train Loss: 0.0159\n",
      "[Epoch 15] Validation Loss: 0.0139\n",
      "[Epoch 16] Train Loss: 0.0157\n",
      "[Epoch 16] Validation Loss: 0.0045\n",
      "[Epoch 17] Train Loss: 0.0153\n",
      "[Epoch 17] Validation Loss: 0.0157\n",
      "[Epoch 18] Train Loss: 0.0165\n",
      "[Epoch 18] Validation Loss: 0.0065\n",
      "[Epoch 19] Train Loss: 0.0148\n",
      "[Epoch 19] Validation Loss: 0.0079\n",
      "[Epoch 20] Train Loss: 0.0157\n",
      "[Epoch 20] Validation Loss: 0.0130\n",
      "[Epoch 21] Train Loss: 0.0152\n",
      "[Epoch 21] Validation Loss: 0.0078\n",
      "[Epoch 22] Train Loss: 0.0145\n",
      "[Epoch 22] Validation Loss: 0.0050\n",
      "[Epoch 23] Train Loss: 0.0143\n",
      "[Epoch 23] Validation Loss: 0.0064\n",
      "[Epoch 24] Train Loss: 0.0147\n",
      "[Epoch 24] Validation Loss: 0.0097\n",
      "[Epoch 25] Train Loss: 0.0137\n",
      "[Epoch 25] Validation Loss: 0.0048\n",
      "[Epoch 26] Train Loss: 0.0148\n",
      "[Epoch 26] Validation Loss: 0.0094\n",
      "[Epoch 27] Train Loss: 0.0133\n",
      "[Epoch 27] Validation Loss: 0.0052\n",
      "[Epoch 28] Train Loss: 0.0139\n",
      "[Epoch 28] Validation Loss: 0.0059\n",
      "[Epoch 29] Train Loss: 0.0143\n",
      "[Epoch 29] Validation Loss: 0.0044\n",
      "[Epoch 30] Train Loss: 0.0140\n",
      "[Epoch 30] Validation Loss: 0.0076\n",
      "[Epoch 31] Train Loss: 0.0142\n",
      "[Epoch 31] Validation Loss: 0.0053\n",
      "[Epoch 32] Train Loss: 0.0138\n",
      "[Epoch 32] Validation Loss: 0.0042\n",
      "[Epoch 33] Train Loss: 0.0135\n",
      "[Epoch 33] Validation Loss: 0.0044\n",
      "[Epoch 34] Train Loss: 0.0131\n",
      "[Epoch 34] Validation Loss: 0.0046\n",
      "[Epoch 35] Train Loss: 0.0136\n",
      "[Epoch 35] Validation Loss: 0.0043\n",
      "[Epoch 36] Train Loss: 0.0131\n",
      "[Epoch 36] Validation Loss: 0.0046\n",
      "[Epoch 37] Train Loss: 0.0128\n",
      "[Epoch 37] Validation Loss: 0.0047\n",
      "[Epoch 38] Train Loss: 0.0126\n",
      "[Epoch 38] Validation Loss: 0.0045\n",
      "[Epoch 39] Train Loss: 0.0140\n",
      "[Epoch 39] Validation Loss: 0.0064\n",
      "[Epoch 40] Train Loss: 0.0134\n",
      "[Epoch 40] Validation Loss: 0.0055\n",
      "[Epoch 41] Train Loss: 0.0133\n",
      "[Epoch 41] Validation Loss: 0.0051\n",
      "[Epoch 42] Train Loss: 0.0133\n",
      "[Epoch 42] Validation Loss: 0.0045\n",
      "[Epoch 43] Train Loss: 0.0134\n",
      "[Epoch 43] Validation Loss: 0.0055\n",
      "[Epoch 44] Train Loss: 0.0142\n",
      "[Epoch 44] Validation Loss: 0.0065\n",
      "[Epoch 45] Train Loss: 0.0135\n",
      "[Epoch 45] Validation Loss: 0.0044\n",
      "[Epoch 46] Train Loss: 0.0128\n",
      "[Epoch 46] Validation Loss: 0.0047\n",
      "[Epoch 47] Train Loss: 0.0136\n",
      "[Epoch 47] Validation Loss: 0.0043\n",
      "[Epoch 48] Train Loss: 0.0126\n",
      "[Epoch 48] Validation Loss: 0.0043\n",
      "[Epoch 49] Train Loss: 0.0130\n",
      "[Epoch 49] Validation Loss: 0.0052\n",
      "[Epoch 50] Train Loss: 0.0130\n",
      "[Epoch 50] Validation Loss: 0.0046\n",
      "[TEST DATA]\n",
      "Test Loss (Average): 0.12\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Next_Open 추가 시 Test Loss가 13.04에서 0.12로 떨어진 이유\n",
    "\n",
    "시계열 데이터에서 next_open은 목표 값인 Close와 강한 상관관계를 가지며, 이를 통해 모델이 데이터의 패턴과 의존성을 더 효과적으로 학습할 수 있다. 이 피처가 없으면 모델이 학습해야 할 데이터의 불확실성이 증가해 손실 값이 커지고 예측 성능이 저하된다. 반면 next_open이 포함되면 모델이 더 구체적이고 직관적인 추론이 가능해져 손실 값이 크게 감소하며, 예측 정확도가 크게 향상된다. 따라서 금융 데이터 예측에서 이와 같은 추가적인 피처는 데이터의 가치를 증대시키는 핵심적인 요소로 작용하기에 next_open을 입력 feature로 추가했을 때의 Loss가 눈에 띄게 감소한 것이다."
   ],
   "id": "50d6d4a5c3a9cf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T02:44:14.752664Z",
     "start_time": "2024-12-20T02:44:14.742654Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "15935bd83d20b96",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
