{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-24T11:13:53.629215Z",
     "start_time": "2024-10-24T11:13:53.602833Z"
    }
   },
   "source": [
    "import os, torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "'''\n",
    "All Dataset class\n",
    "'''\n",
    "# 타이타닉 \"전체 데이터셋\" 사용 위한 클래스\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y): # Initial Setting\n",
    "        self.X = torch.FloatTensor(X) # 입력 데이터\n",
    "        # 입력 데이터를 부동 소수점 형태로 변환하는 이유는\n",
    "        # 입력 데이터가 모델의 가중치와 곱셈 연산을 수행해\n",
    "        # 소수점 정밀도가 필요하기 때문이다.\n",
    "        \n",
    "        self.y = torch.LongTensor(y) # 출력 데이터\n",
    "        # 출력 데이터는 입력 데이터와 달리\n",
    "        # 별다른 계산을 수행하지 않고 분류만 하므로\n",
    "        # 정밀도가 필요하지 않고 정수를 사용하는 것이 효과적이다.\n",
    "    \n",
    "    # 데이터셋 sample 수 반환\n",
    "    def __len__(self): return len(self.X)\n",
    "    \n",
    "    # 주어진 idx에 해당하는 샘플을 가져옴\n",
    "    def __getitem__(self, idx):\n",
    "        feature, target = self.X[idx], self.y[idx]\n",
    "        return {'input': feature, 'target': target} #딕셔너리로 반환\n",
    "\n",
    "    # dataset String 표현\n",
    "    def __str__(self):\n",
    "        # 데이터 크기, 입력 형태, 타겟 형태 출력\n",
    "        str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.X), self.X.shape, self.y.shape\n",
    "        )\n",
    "        return str"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:13:53.660509Z",
     "start_time": "2024-10-24T11:13:53.644369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Test Dataset Class\n",
    "테스트용이므로 정답 레이블(target)을 포함하지 않음\n",
    "'''\n",
    "# 타이타닉 \"테스트 데이터셋\" 사용 위한 클래스 \n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X): # Initial Setting\n",
    "        self.X = torch.FloatTensor(X) # 입력 데이터\n",
    "    \n",
    "    def __len__(self): return len(self.X) # 데이터셋 sample 수 반환\n",
    "\n",
    "    # 주어진 idx에 해당하는 샘플을 가져옴\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        return {'input': feature} #딕셔너리로 반환\n",
    "\n",
    "    # dataset String 표현\n",
    "    def __str__(self):\n",
    "        # 데이터 크기, 입력 형태, 타겟 형태 출력\n",
    "        str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "            len(self.X), self.X.shape\n",
    "        )\n",
    "        return str"
   ],
   "id": "f7dec973dbcdc059",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:13:53.723545Z",
     "start_time": "2024-10-24T11:13:53.695777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_preprocessed_dataset():\n",
    "    # Load the paths of datasets\n",
    "    # CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__)) # 주피터 노트북에서는 __file__ 허용되지 않음\n",
    "    CURRENT_FILE_PATH = os.getcwd()  # 현재 작업 디렉토리 경로 가져오기\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "    \n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(train_data_path) # train dataset\n",
    "    test_df = pd.read_csv(test_data_path) # test dataset\n",
    "    \n",
    "    # train + test (No sort)\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "    \n",
    "    # Preprocessing\n",
    "    all_df = get_preprocessed_dataset_1(all_df) # 1회\n",
    "    all_df = get_preprocessed_dataset_2(all_df) # 2회\n",
    "    all_df = get_preprocessed_dataset_3(all_df) # 3회\n",
    "    all_df = get_preprocessed_dataset_4(all_df) # 4회\n",
    "    all_df = get_preprocessed_dataset_5(all_df) # 5회\n",
    "    all_df = get_preprocessed_dataset_6(all_df) # 6회\n",
    "    \n",
    "    # 학습 및 테스트 데이터 분리\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    #print(test_dataset)\n",
    "    # 학습 데이터셋을 생성하고 훈련 및 검증 데이터셋으로 분할.\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    #print(dataset)\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    # 테스트 데이터셋 생성\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ],
   "id": "cf8888fc42142af",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:13:53.804165Z",
     "start_time": "2024-10-24T11:13:53.768434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Preprocessing\n",
    "'''\n",
    "# 여러 단계의 데이터 전처리 함수 정의\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    # Pclass별로 Fare 평균값을 사용하여 결측치를 채움\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # Name 컬럼을 family_name, honorific, name으로 분리하여 추가합니다.\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # honorific별로 Age 평균값을 사용하여 결측치를 채움\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index()\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\"]\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"]\n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # 가족 수와 혼자 탑승 여부 컬럼 추가\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "\n",
    "    # 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # honorific 컬럼의 값 개수 줄이기\n",
    "    all_df.loc[\n",
    "        ~(\n",
    "            (all_df[\"honorific\"] == \"Mr\") |\n",
    "            (all_df[\"honorific\"] == \"Miss\") |\n",
    "            (all_df[\"honorific\"] == \"Mrs\") |\n",
    "            (all_df[\"honorific\"] == \"Master\")\n",
    "        ),\n",
    "        \"honorific\"\n",
    "    ] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "    return all_df\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # 카테고리 변수들을 LabelEncoder를 사용하여 수치형으로 변환\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "            le = le.fit(all_df[category_feature])\n",
    "            all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "    return all_df"
   ],
   "id": "6f0b39331bf11150",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:13:53.900698Z",
     "start_time": "2024-10-24T11:13:53.844802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn, optim\n",
    "'''\n",
    "사용자 정의 신경망 모델\n",
    "'''\n",
    "class ActivationFunc(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # activate function 정의\n",
    "        self.activate_dir_ = {'ReLU' : nn.ReLU,\n",
    "                         'Leaky ReLU' : nn.LeakyReLU,\n",
    "                        'Sigmoid' : nn.Sigmoid,\n",
    "                        'Tanh' : nn.Tanh,\n",
    "                        'SoftPlus' : nn.Softplus,\n",
    "                        'ELU' : nn.ELU,\n",
    "                        'GELU' : nn.GELU,\n",
    "                        'SiLU' : nn.SiLU,\n",
    "                        'Mish' : nn.Mish}\n",
    "        \n",
    "        self.activate_list_ = list(self.activate_dir_.values())\n",
    "    \n",
    "    def get_activation_func(self, name = None, idx = None) :\n",
    "        if (name is not None) : return self.activate_dir_[name]()\n",
    "        elif (idx is not None) : return self.activate_list_[idx]()\n",
    "        else : return nn.ReLU() #Default\n",
    "\n",
    "# 사용자 정의 신경망 모델 클래스 정의\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output):  # 초기 설정\n",
    "        super().__init__()  # 부모 클래스 초기화\n",
    "        actFunc_ = ActivationFunc()\n",
    "        # 모델 구조 정의 (은닉층 2개 포함)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 30),  # 입력층 -> 첫 번째 은닉층\n",
    "            # nn.ReLU(),  # 활성화 함수: ReLU\n",
    "            actFunc_.get_activation_func(name = 'ELU'),\n",
    "            nn.Linear(30, 30),  # 첫 번째 은닉층 -> 두 번째 은닉층\n",
    "            # nn.ReLU(),  # 활성화 함수: ReLU\n",
    "            actFunc_.get_activation_func(name = 'ELU'),\n",
    "            nn.Linear(30, n_output),  # 두 번째 은닉층 -> 출력층\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):  # 순전파 함수\n",
    "        return self.model(x)  # 정의된 모델을 통해 순전파 수행\n",
    "    \n",
    "# 모델 및 옵티마이저를 가져오는 함수\n",
    "def get_model_and_optimizer(input_size, output_size):\n",
    "    model = MyModel(input_size, output_size)  # 사용자 정의 모델 인스턴스 생성\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)  # 옵티마이저: 확률적 경사 하강법 (학습률 0.01)\n",
    "    return model, optimizer\n"
   ],
   "id": "1507b4432e4fd0a8",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:13:54.041330Z",
     "start_time": "2024-10-24T11:13:53.970908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "# 학습 루프 함수\n",
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    #n_epochs = wandb.config.epochs  # 에포크 수를 wandb 설정에서 가져옴\n",
    "    n_epochs = 700\n",
    "    loss_fn = nn.CrossEntropyLoss()  # 손실 함수로 교차 엔트로피 손실 사용\n",
    "    next_print_epoch = 100  # 다음 출력할 에포크 설정\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()  # 모델을 학습 모드로 설정\n",
    "        loss_train = 0.0  # 학습 손실 초기화\n",
    "        correct_train = 0  # 정확히 예측한 학습 샘플 수 초기화\n",
    "        total_train = 0  # 전체 학습 샘플 수 초기화\n",
    "        for batch in train_data_loader:  # 학습 데이터 로더에서 배치 반복\n",
    "            inputs, targets = batch['input'], batch['target']  # 입력과 타겟 데이터 가져오기\n",
    "            outputs = model(inputs)  # 모델을 사용해 예측값 계산\n",
    "            loss = loss_fn(outputs, targets)  # 손실 함수 계산\n",
    "            loss_train += loss.item()  # 손실 누적\n",
    "\n",
    "            optimizer.zero_grad()  # 경사 초기화\n",
    "            loss.backward()  # 역전파를 통해 경사 계산\n",
    "            optimizer.step()  # 경사를 사용해 가중치 업데이트\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  # 가장 높은 값을 가지는 클래스 예측\n",
    "            correct_train += (predicted == targets).sum().item()  # 정확히 예측한 개수 누적\n",
    "            total_train += targets.size(0)  # 전체 샘플 수 누적\n",
    "\n",
    "        # 검증 손실 및 정확도 계산\n",
    "        loss_validation = 0.0  # 검증 손실 초기화\n",
    "        correct_val = 0  # 정확히 예측한 검증 샘플 수 초기화\n",
    "        total_val = 0  # 전체 검증 샘플 수 초기화\n",
    "        model.eval()  # 모델을 평가 모드로 설정\n",
    "        with torch.no_grad():  # 경사 계산 비활성화 (평가 시 필요 없음)\n",
    "            for batch in validation_data_loader:  # 검증 데이터 로더에서 배치 반복\n",
    "                inputs, targets = batch['input'], batch['target']  # 입력과 타겟 데이터 가져오기\n",
    "                outputs = model(inputs)  # 모델을 사용해 예측값 계산\n",
    "                loss = loss_fn(outputs, targets)  # 손실 함수 계산\n",
    "                loss_validation += loss.item()  # 손실 누적\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)  # 가장 높은 값을 가지는 클래스 예측\n",
    "                correct_val += (predicted == targets).sum().item()  # 정확히 예측한 개수 누적\n",
    "                total_val += targets.size(0)  # 전체 샘플 수 누적\n",
    "\n",
    "        train_accuracy = correct_train / total_train  # 학습 정확도 계산\n",
    "        val_accuracy = correct_val / total_val  # 검증 정확도 계산\n",
    "\n",
    "        # Wandb에 로그 기록\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,  # 현재 에포크\n",
    "            \"Training loss\": loss_train / len(train_data_loader),  # 에포크당 평균 학습 손실\n",
    "            \"Validation loss\": loss_validation / len(validation_data_loader)  # 에포크당 평균 검증 손실\n",
    "        })\n",
    "\n",
    "        # 특정 에포크마다 학습 및 검증 손실 출력\n",
    "        if epoch >= next_print_epoch:\n",
    "            print(\n",
    "                f\"Epoch {epoch}, \"\n",
    "                f\"Training loss {loss_train / len(train_data_loader):.4f}, \"\n",
    "                f\"Validation loss {loss_validation / len(validation_data_loader):.4f}\"\n",
    "            )\n",
    "            # print(f\"Testing at epoch {epoch}\")\n",
    "            # test_model(model, test_data_loader)\n",
    "            next_print_epoch += 100\n"
   ],
   "id": "2030b7891e264c96",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:13:54.151396Z",
     "start_time": "2024-10-24T11:13:54.106039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Test\n",
    "'''\n",
    "# 테스트 함수\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    print(\"[TEST]\")\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = batch['input']  # 입력 데이터 가져오기\n",
    "            outputs = model(inputs)  # 모델 출력 계산\n",
    "            predictions = torch.argmax(outputs, dim=1)  # 예측값 계산\n",
    "            # 예측 결과 출력 및 저장 (인덱스는 892부터 시작)\n",
    "            for idx, prediction in enumerate(predictions, start=892):\n",
    "                print(idx, prediction.item())\n",
    "                results.append({'PassengerId': idx, 'Survived': prediction.item()})\n",
    "            # 테스트 데이터에 대해 정확도 평가 (타겟 값은 없으므로 실제 예측 정확도 계산 불가)\n",
    "            correct_test += (predictions == 1).sum().item()  # 임시로 1로 설정된 기준에 따른 평가\n",
    "            total_test += predictions.size(0)  # 전체 샘플 수 누적\n",
    "    test_accuracy = correct_test / total_test  # 테스트 정확도 계산\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # 현재 작업 디렉토리를 출력하여 확인\n",
    "    current_directory = os.getcwd()\n",
    "    print(f\"Current working directory: {current_directory}\")\n",
    "\n",
    "    # 절대 경로로 파일을 저장\n",
    "    results_df = pd.DataFrame(results)\n",
    "    file_path = os.path.join(current_directory, 'submission.csv')\n",
    "    results_df.to_csv(file_path, index=False)\n",
    "    print(f\"Test results saved to {file_path}\")\n"
   ],
   "id": "73277904171484e1",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T11:14:58.654058Z",
     "start_time": "2024-10-24T11:13:54.222297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Main Function\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    # Wandb 설정 초기화\n",
    "    current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')  # 현재 시간 문자열로 변환\n",
    "\n",
    "    config = {\n",
    "        'epochs': 1000,  # 총 학습 에포크 수\n",
    "        'batch_size': 16,  # 배치 크기\n",
    "        'learning_rate': 1e-3,  # 학습률\n",
    "    }\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\",  # 온라인 모드로 설정 (wandb 로그 기록 활성화)\n",
    "        project=\"titanic_model_training_ELU\",  # 프로젝트 이름\n",
    "        notes=\"Titanic dataset training with wandb\",  # 실험에 대한 설명\n",
    "        tags=[\"titanic\", \"classification\"],  # 태그 설정\n",
    "        name=current_time_str,  # 실험 이름 (현재 시간)\n",
    "        config=config  # 설정값 전달\n",
    "    )\n",
    "\n",
    "    # 데이터셋 로드 및 분할\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()  # 전처리된 데이터셋 가져오기\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)  # 학습 데이터 로더\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))  # 검증 데이터 로더\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))  # 테스트 데이터 로더\n",
    "    \n",
    "    # 모델 및 옵티마이저 정의\n",
    "    input_size = train_dataset[0]['input'].shape[0]  # 입력 특성 수\n",
    "    output_size = 2  # 출력 클래스 수 (생존 여부: 0 또는 1)\n",
    "    model = MyModel(input_size, output_size)  # 모델 인스턴스 생성\n",
    "    optimizer = optim.SGD(model.parameters(), lr=wandb.config.learning_rate)  # 옵티마이저 설정 (SGD)\n",
    "\n",
    "    # 학습 루프 실행\n",
    "    training_loop(model, optimizer, train_data_loader, validation_data_loader)\n",
    "\n",
    "    # 테스트 데이터셋에 대해 모델 평가\n",
    "    test_model(model, test_data_loader)\n",
    "    \n",
    "    wandb.finish()  # wandb 세션 종료\n",
    "    "
   ],
   "id": "de6807503d9280f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\yscho\\git\\link_dl\\_02_homeworks\\homework_2\\wandb\\run-20241024_201354-vyyuxsb9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU/runs/vyyuxsb9' target=\"_blank\">2024-10-24_20-13-54</a></strong> to <a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU' target=\"_blank\">https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU/runs/vyyuxsb9' target=\"_blank\">https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU/runs/vyyuxsb9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yscho\\AppData\\Local\\Temp\\ipykernel_17252\\3076189289.py:36: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\yscho\\AppData\\Local\\Temp\\ipykernel_17252\\3076189289.py:53: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.5519, Validation loss 0.5531\n",
      "Epoch 200, Training loss 0.5155, Validation loss 0.5683\n",
      "Epoch 300, Training loss 0.4863, Validation loss 0.5532\n",
      "Epoch 400, Training loss 0.4488, Validation loss 0.4979\n",
      "Epoch 500, Training loss 0.4301, Validation loss 0.5000\n",
      "Epoch 600, Training loss 0.4251, Validation loss 0.4841\n",
      "Epoch 700, Training loss 0.4140, Validation loss 0.5534\n",
      "[TEST]\n",
      "892 0\n",
      "893 0\n",
      "894 0\n",
      "895 0\n",
      "896 0\n",
      "897 0\n",
      "898 0\n",
      "899 0\n",
      "900 1\n",
      "901 0\n",
      "902 0\n",
      "903 0\n",
      "904 1\n",
      "905 0\n",
      "906 0\n",
      "907 1\n",
      "908 0\n",
      "909 0\n",
      "910 0\n",
      "911 0\n",
      "912 0\n",
      "913 0\n",
      "914 1\n",
      "915 0\n",
      "916 0\n",
      "917 0\n",
      "918 1\n",
      "919 0\n",
      "920 0\n",
      "921 0\n",
      "922 0\n",
      "923 0\n",
      "924 0\n",
      "925 0\n",
      "926 0\n",
      "927 0\n",
      "928 0\n",
      "929 0\n",
      "930 0\n",
      "931 1\n",
      "932 0\n",
      "933 0\n",
      "934 0\n",
      "935 1\n",
      "936 0\n",
      "937 0\n",
      "938 0\n",
      "939 0\n",
      "940 0\n",
      "941 0\n",
      "942 0\n",
      "943 0\n",
      "944 0\n",
      "945 0\n",
      "946 0\n",
      "947 0\n",
      "948 0\n",
      "949 0\n",
      "950 0\n",
      "951 1\n",
      "952 0\n",
      "953 0\n",
      "954 0\n",
      "955 0\n",
      "956 0\n",
      "957 0\n",
      "958 0\n",
      "959 0\n",
      "960 0\n",
      "961 0\n",
      "962 0\n",
      "963 0\n",
      "964 0\n",
      "965 0\n",
      "966 1\n",
      "967 0\n",
      "968 0\n",
      "969 0\n",
      "970 0\n",
      "971 0\n",
      "972 0\n",
      "973 0\n",
      "974 0\n",
      "975 0\n",
      "976 0\n",
      "977 0\n",
      "978 0\n",
      "979 0\n",
      "980 0\n",
      "981 1\n",
      "982 0\n",
      "983 0\n",
      "984 1\n",
      "985 0\n",
      "986 0\n",
      "987 0\n",
      "988 0\n",
      "989 0\n",
      "990 0\n",
      "991 0\n",
      "992 0\n",
      "993 0\n",
      "994 0\n",
      "995 0\n",
      "996 1\n",
      "997 0\n",
      "998 0\n",
      "999 0\n",
      "1000 0\n",
      "1001 0\n",
      "1002 0\n",
      "1003 0\n",
      "1004 0\n",
      "1005 0\n",
      "1006 0\n",
      "1007 0\n",
      "1008 0\n",
      "1009 1\n",
      "1010 0\n",
      "1011 0\n",
      "1012 1\n",
      "1013 0\n",
      "1014 1\n",
      "1015 0\n",
      "1016 0\n",
      "1017 1\n",
      "1018 0\n",
      "1019 0\n",
      "1020 0\n",
      "1021 0\n",
      "1022 0\n",
      "1023 0\n",
      "1024 0\n",
      "1025 0\n",
      "1026 0\n",
      "1027 0\n",
      "1028 0\n",
      "1029 0\n",
      "1030 0\n",
      "1031 0\n",
      "1032 0\n",
      "1033 1\n",
      "1034 0\n",
      "1035 0\n",
      "1036 0\n",
      "1037 0\n",
      "1038 0\n",
      "1039 0\n",
      "1040 0\n",
      "1041 0\n",
      "1042 1\n",
      "1043 0\n",
      "1044 0\n",
      "1045 0\n",
      "1046 0\n",
      "1047 0\n",
      "1048 1\n",
      "1049 0\n",
      "1050 0\n",
      "1051 1\n",
      "1052 0\n",
      "1053 0\n",
      "1054 1\n",
      "1055 0\n",
      "1056 0\n",
      "1057 0\n",
      "1058 0\n",
      "1059 0\n",
      "1060 1\n",
      "1061 0\n",
      "1062 0\n",
      "1063 0\n",
      "1064 0\n",
      "1065 0\n",
      "1066 0\n",
      "1067 1\n",
      "1068 1\n",
      "1069 0\n",
      "1070 0\n",
      "1071 0\n",
      "1072 0\n",
      "1073 0\n",
      "1074 1\n",
      "1075 0\n",
      "1076 0\n",
      "1077 0\n",
      "1078 1\n",
      "1079 0\n",
      "1080 0\n",
      "1081 0\n",
      "1082 0\n",
      "1083 0\n",
      "1084 0\n",
      "1085 0\n",
      "1086 1\n",
      "1087 0\n",
      "1088 1\n",
      "1089 0\n",
      "1090 0\n",
      "1091 0\n",
      "1092 0\n",
      "1093 1\n",
      "1094 0\n",
      "1095 1\n",
      "1096 0\n",
      "1097 0\n",
      "1098 0\n",
      "1099 0\n",
      "1100 1\n",
      "1101 0\n",
      "1102 0\n",
      "1103 0\n",
      "1104 0\n",
      "1105 0\n",
      "1106 0\n",
      "1107 0\n",
      "1108 0\n",
      "1109 1\n",
      "1110 0\n",
      "1111 0\n",
      "1112 1\n",
      "1113 0\n",
      "1114 1\n",
      "1115 0\n",
      "1116 0\n",
      "1117 0\n",
      "1118 0\n",
      "1119 0\n",
      "1120 0\n",
      "1121 0\n",
      "1122 0\n",
      "1123 1\n",
      "1124 0\n",
      "1125 0\n",
      "1126 0\n",
      "1127 0\n",
      "1128 0\n",
      "1129 0\n",
      "1130 1\n",
      "1131 0\n",
      "1132 0\n",
      "1133 0\n",
      "1134 0\n",
      "1135 0\n",
      "1136 0\n",
      "1137 0\n",
      "1138 1\n",
      "1139 0\n",
      "1140 0\n",
      "1141 0\n",
      "1142 1\n",
      "1143 0\n",
      "1144 0\n",
      "1145 0\n",
      "1146 0\n",
      "1147 0\n",
      "1148 0\n",
      "1149 0\n",
      "1150 1\n",
      "1151 0\n",
      "1152 0\n",
      "1153 0\n",
      "1154 1\n",
      "1155 1\n",
      "1156 0\n",
      "1157 0\n",
      "1158 0\n",
      "1159 0\n",
      "1160 0\n",
      "1161 0\n",
      "1162 0\n",
      "1163 0\n",
      "1164 0\n",
      "1165 0\n",
      "1166 0\n",
      "1167 1\n",
      "1168 0\n",
      "1169 0\n",
      "1170 0\n",
      "1171 0\n",
      "1172 0\n",
      "1173 1\n",
      "1174 0\n",
      "1175 1\n",
      "1176 1\n",
      "1177 0\n",
      "1178 0\n",
      "1179 0\n",
      "1180 0\n",
      "1181 0\n",
      "1182 0\n",
      "1183 0\n",
      "1184 0\n",
      "1185 0\n",
      "1186 0\n",
      "1187 0\n",
      "1188 1\n",
      "1189 0\n",
      "1190 0\n",
      "1191 0\n",
      "1192 0\n",
      "1193 0\n",
      "1194 0\n",
      "1195 0\n",
      "1196 0\n",
      "1197 0\n",
      "1198 0\n",
      "1199 1\n",
      "1200 0\n",
      "1201 0\n",
      "1202 0\n",
      "1203 0\n",
      "1204 0\n",
      "1205 0\n",
      "1206 1\n",
      "1207 0\n",
      "1208 1\n",
      "1209 0\n",
      "1210 0\n",
      "1211 0\n",
      "1212 0\n",
      "1213 0\n",
      "1214 0\n",
      "1215 0\n",
      "1216 1\n",
      "1217 0\n",
      "1218 0\n",
      "1219 0\n",
      "1220 0\n",
      "1221 0\n",
      "1222 0\n",
      "1223 0\n",
      "1224 0\n",
      "1225 1\n",
      "1226 0\n",
      "1227 0\n",
      "1228 0\n",
      "1229 0\n",
      "1230 0\n",
      "1231 0\n",
      "1232 0\n",
      "1233 0\n",
      "1234 0\n",
      "1235 1\n",
      "1236 1\n",
      "1237 0\n",
      "1238 0\n",
      "1239 0\n",
      "1240 0\n",
      "1241 0\n",
      "1242 0\n",
      "1243 0\n",
      "1244 0\n",
      "1245 0\n",
      "1246 1\n",
      "1247 0\n",
      "1248 0\n",
      "1249 0\n",
      "1250 0\n",
      "1251 0\n",
      "1252 0\n",
      "1253 0\n",
      "1254 0\n",
      "1255 0\n",
      "1256 1\n",
      "1257 0\n",
      "1258 0\n",
      "1259 1\n",
      "1260 0\n",
      "1261 0\n",
      "1262 0\n",
      "1263 1\n",
      "1264 0\n",
      "1265 0\n",
      "1266 0\n",
      "1267 1\n",
      "1268 0\n",
      "1269 0\n",
      "1270 0\n",
      "1271 0\n",
      "1272 0\n",
      "1273 0\n",
      "1274 0\n",
      "1275 0\n",
      "1276 0\n",
      "1277 0\n",
      "1278 0\n",
      "1279 0\n",
      "1280 0\n",
      "1281 0\n",
      "1282 1\n",
      "1283 0\n",
      "1284 0\n",
      "1285 0\n",
      "1286 0\n",
      "1287 1\n",
      "1288 0\n",
      "1289 0\n",
      "1290 0\n",
      "1291 0\n",
      "1292 1\n",
      "1293 0\n",
      "1294 1\n",
      "1295 0\n",
      "1296 0\n",
      "1297 0\n",
      "1298 0\n",
      "1299 0\n",
      "1300 0\n",
      "1301 1\n",
      "1302 0\n",
      "1303 1\n",
      "1304 0\n",
      "1305 0\n",
      "1306 1\n",
      "1307 0\n",
      "1308 0\n",
      "1309 1\n",
      "Test accuracy: 0.1579\n",
      "Current working directory: C:\\Users\\yscho\\git\\link_dl\\_02_homeworks\\homework_2\n",
      "Test results saved to C:\\Users\\yscho\\git\\link_dl\\_02_homeworks\\homework_2\\submission.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60b80b4d92b04215ab1655ec3c3131b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>Training loss</td><td>███▇▇▇▇▇▆▆▆▆▅▅▄▄▄▄▄▃▃▄▃▂▂▃▂▁▂▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>Validation loss</td><td>▇██▇▆▆▆▅▇▅▅▅▄▆▆▄▃▄▅▃▆▅▅▃▂▃▄▄▂▃▂▁▄▄▂█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>700</td></tr><tr><td>Training loss</td><td>0.41398</td></tr><tr><td>Validation loss</td><td>0.55344</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2024-10-24_20-13-54</strong> at: <a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU/runs/vyyuxsb9' target=\"_blank\">https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU/runs/vyyuxsb9</a><br/> View project at: <a href='https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU' target=\"_blank\">https://wandb.ai/yuuun03-korea-university-of-technology-and-education/titanic_model_training_ELU</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241024_201354-vyyuxsb9\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 결과\n",
    "![Activation Function Image](https://raw.githubusercontent.com/yuuun03/DL_choyunseo_3_2_/dacb1ee9748c98a77aa3ac3bc41b7bbfae10ef96/wandb_activationFunc.png)\n",
    "\n",
    "본 그래프에서 각 시간-색깔이 의미하는 활성화 함수는 다음과 같다.\n",
    "1. Mish : 2024-10-18_09-18-42 (파란색)\n",
    "2. ReLU : 2024-10-18_18-23-14 (빨간색)\n",
    "3. Leaky ReLU : 2024-10-18_18-27-22 (초록색)\n",
    "4. Sigmoid : 2024-10-18_18-30-03 (보라색)\n",
    "5. Tanh : 2024-10-18_18-33-15 (분홍색)\n",
    "6. Softplus : 2024-10-18_18-35-34 (주황색)\n",
    "7. ELU : 2024-10-18_18-37-39 (민트색)\n",
    "8. GELU : 2024-10-18_18-39-22 (자주색)\n",
    "9. SiLU : 2024-10-18_18-42-49 (노란색)\n",
    "\n",
    "Validation Loss가 낮고 안정적일 수록 성능이 좋은 것이므로 성능이 좋은 순서대로 활성화함수를 다시 나열하면 다음과 같다. \n",
    "1. **<span style=\"color: #3EB489;\">ELU : 2024-10-18_18-37-39 (민트색)</span>**\n",
    "2. SiLU : 2024-10-18_18-42-49 (노란색)\n",
    "3. ReLU : 2024-10-18_18-23-14 (빨간색)\n",
    "4. GELU : 2024-10-18_18-39-22 (자주색)\n",
    "5. Tanh : 2024-10-18_18-33-15 (분홍색)\n",
    "6. Mish : 2024-10-18_09-18-42 (파란색)\n",
    "7. Softplus : 2024-10-18_18-35-34 (주황색)\n",
    "8. Leaky ReLU : 2024-10-18_18-27-22 (초록색)\n",
    "9. **<span style=\"color: #8E44AD;\">Sigmoid : 2024-10-18_18-30-03 (보라색)</span>**\n",
    "\n",
    "\n",
    "# 고찰\n",
    "## 제일 성능이 좋았던 함수 : ELU\n",
    "음수 입력에 대해 선형적으로 비선형성을 적용하여 기울기 소실 문제를 완화하는 것이 ELU의 특징이다. 또한 학습 속도가 빠르기 때문에 성능이 높게 나올 수 있다.\n",
    "## 제일 성능이 안 좋았던 함수 : Sigmoid\n",
    "Sigmoid 함수는 출력이 (0, 1) 사이로 제한되어 있고, 큰 입력값에서 기울기가 매우 작아져 기울기 소실 문제가 발생하기 쉽다. 이 때문에 학습이 느리며 복잡한 비선형 문제에서 성능이 떨어진다.\n",
    "\n",
    "# Epoch에 대한 고찰\n",
    "### Epoch 700일 때 멈추는 것이 가장 낫다.\n",
    "[ELU 사용.]\n",
    "**가장 낮은 Validation loss는 Epoch 500**일 때이나 **Test Accuracy가 가장 높을 때는 Epoch 700**일 때이다. 두 에폭 간의 Validation loss의 차이가 그렇게 크지 않으므로 Test Accuracy의 높음 정도를 더 중요시 하는 것이 옳다고 판단하였다. 다만 에폭 700 이후에는 Validation loss도 Test Accuracy도 크게 개선되는 점이 없으므로 이 때 멈추는 것이 가장 좋다고 생각하였다.\n"
   ],
   "id": "a4f968074254eb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Kaggle 결과\n",
    "![Kaggle결과_조윤서](https://github.com/yuuun03/DL_choyunseo_3_2_/raw/c0bd4852bd042178bee2ed00a6c5501c7ff6a51d/kaggle_leaderboard_2022136117%EC%A1%B0%EC%9C%A4%EC%84%9C.png)"
   ],
   "id": "fea0dc60b762266e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "31b762c74d2cb6ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 숙제 후기\n",
    "타이타닉 데이터셋은 스크립트 프로그래밍과 머신러닝 때도 사용해보았던 데이터셋입니다. 그 당시에는 그냥 pandas, seaborn을 사용한 가공 및 시각화를 하거나 직접 numpy로만 MLP를 이용하여서 분석해보았었는데 이번에 딥러닝 시간에 현업에서 주로 사용하는 딥러닝 모듈(또는 사이트)을 사용하여 데이터를 분석해보아서 좋았습니다. 그러면서 Kaggle, WandB라는 새로운 AI 사이트를 알게 되어 좋았습니다. 특히 WandB의 경우 사용해보니 너무 간편하게 시각화를 해주어 앞으로도 딥러닝을 이용한 개발을 하게 된다면 유용하게 사용할 것 같습니다."
   ],
   "id": "a290bc7ced1de83a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e9b80c4224d643b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
